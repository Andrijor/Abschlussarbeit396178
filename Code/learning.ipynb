{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"../Models/roberta-base-squad2\"\n",
    "#model_name = \"../Models/distilbert-base-cased-distilled-squad\"\n",
    "#model = TFAutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.04215247184038162,\n",
       " 'start': 1351,\n",
       " 'end': 1371,\n",
       " 'answer': 'Inter-agent chatbots'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'What kinds of chatbots are there?',\n",
    "    #'context': 'BERT, or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) takss.  It’s safe to say it is taking the NLP world by storm. BERT was developed by Google and Nvidia has created an optimized version that uses TensorRT.'\n",
    "    'context': \"Types of Chatbots\\nChatbots can be classified using different parameters: the knowledge domain, the service provided, the goals, the input processing and response generation method, the human-aid, and the build method.\\nClassification based on the knowledge domain considers the knowledge a chatbot can access or the amount of data it is trained upon. Open domain chatbots can talk about general topics and respond appropriately, while closed domain chatbots are focused on a particular knowledge domain and might fail to respond to other questions.\\nClassification based on the service provided considers the sentimental proximity of the chatbot to the user, the amount of intimate interaction that takes place, and it is also dependent upon the task the chatbot is performing. Interpersonal chatbots lie in the domain of communication and provide services such as Restaurant booking, Flight booking, and FAQ bots. They are not companions of the user, but they get information and\\npass them on to the user. They can have a personality, can be friendly, and will probably remember information about the user, but they are not obliged or expected to do so. Intrapersonal chatbots exist within the personal domain of the user, such as chat apps like Messenger, Slack, and WhatsApp. They are companions to the user and understand the user like a human does.Inter-agent chatbots become omnipresent while all chatbots will require some inter-chatbot communication possibilities. The need for protocols for inter-chatbot communication has already emerged. Alexa-Cortana integration is an example of inter-agent communication.\\nClassification based on the goals considers the primary goal chatbots aim to achieve. Informative chatbots are designed to provide the user with information that is stored beforehand or is available from a fixed source, like FAQ chatbots. Chatbased/Conversational chatbots talk to the user, like another human being, and their goal is to respond correctly to the sentence they have been given.Task-based chatbots perform a specific task such as booking a flight or helping somebody. These chatbots are intelligent in the context of asking for information and understanding the user’s input. Restaurant booking bots and FAQ chatbots are examples of Task-based chatbots.\\nClassification based on the input processing and response generation method takes into account the method of processing inputs and generating responses. There are three models used to produce the appropriate responses: rule-based model, retrievalbased model, and generative model.\\nRule-based model chatbots are the type of architecture which most of the first chatbots have been built with, like numerous online chatbots. They choose the system response based on a fixed predefined set of rules, based on recognizing the lexical form of the input text without creating any new text answers. The knowledge used in the chatbot is humanly hand-coded and is organized and presented with conversational patterns. A more comprehensive rule database allows the chatbot to reply to more types of user input. However, this type of model is not robust to spelling and grammatical mistakes in user input. Most existing research on rule-based chatbots studies response selection for single-turn conversation, which only considers the last input message. In more humanlike chatbots, multi-turn response selection takes into consideration previous parts of the conversation to select a response relevant to the whole conversation context.\\nA little different from the rule-based model is the retrieval-based model, which offers more flexibility as it queries and analyzes available resources using APIs. \\nA retrieval-based chatbot retrieves some response candidates from an index before it applies the matching approach to the response selection.\\nThe generative model generates answers in a better way than the other three models, based on current and previous user messages. These chatbots are more human-like An Overview of Chatbot Technology 379 and use machine learning algorithms and deep learning techniques. However, there are difficulties in building and training them.\\nAnother classification for chatbots considers the amount of human-aid in their components. Human-aided chatbots utilize human computation in at least one element from the chatbot. Crowd workers, freelancers, or full-time employees can embody their intelligence in the chatbot logic to fill the gaps caused by limitations of fully automated chatbots. While human computation, compared to rule-based algorithms and machine learning, provides more flexibility and robustness, still, it cannot process a given piece of information as fast as a machine, which makes it hard to scale to more user requests.\\nChatbots can also be classified according to the permissions provided by their development platform. Development platforms can be of open-source, such as RASA, or can be of proprietary code such as development platforms typically offered by large companies such as Google or IBM. Open-source platforms provide the chatbot designer with the ability to intervene in most aspects of implementation. Closed platforms, typically act as black boxes, which may be a significant disadvantage depending on the project requirements. However, access to state-of-the-art technologies may be considered more immediate for large companies. Moreover, one may assume that chatbots developed based on large companies’ platforms may be benefited by a large amount of data that these companies collect.\\nOf course, chatbots do not exclusively belong to one category or another, but these categories exist in each chatbot in varying proportions.\"\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bachelorarbeit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68add61621d5ebe8628a59c51f8b199db9b942fbb130015c9d0f081a7177ca2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
